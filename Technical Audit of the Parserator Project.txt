Technical Audit of the Parserator Project 


Project Overview and Architecture 


Parserator 
is 
a 
structured 
data 
parsing 
framework 
for 
AI 
agents, 
built 
around 
a 
two-stage 
"Architect-
Extractor" 
pattern. 
In 
this 
design, 
two 
LLM 
instances 
collaborate: 
LLM 
1 
("Architect") 
plans 
the 
extraction 
given 
a 
schema 
and 
a 
small 
sample 
of 
input, 
and 
LLM 
2 
("Extractor") 
executes 
that 
plan 
on 
the 
full 
data 


1
2
. 
This 
yields 
high 
accuracy 
(~95%) 
while 
drastically 
reducing 
token 
usage 
(~70% 
less 
than 
single-LLM 
methods) 
3. 
The 
core 
philosophy 
aligns 
with 
Exoditical 
Moral 
Architecture 
(EMA) 
principles, 
emphasizing 
user 
data 
freedom 
and 
system 
openness. 
The 
EMA 
Manifesto 
outlines 
four 
key 
tenets 
– 
User 
Sovereignty 
(ownership 
of 
data), 
Portability 
(first-class 
export), 
Standards 
Agnosticism 
(universal 
formats 
over 
proprietary), 
and 
Transparent 
Competition 
(honest 
comparison, 
no 
lock-in) 
45. 
Parserator’s 
architecture 
and 
workflows 
are 
intentionally 
built 
to 
uphold 
these 
ideals 
(e.g. 
JSON/OpenAPI 
outputs, 
no 
data 
retention, 
easy migration) 


The 
repository 
is 
structured 
as 
a 
monorepo 
containing 
multiple 
components 
of 
the 
platform 
. 
Major 
sub-projects include: 

• Core Engine – the core parsing orchestration and prompt logic (“Architect-Extractor” 
implementation). 
• Node.js SDK (packages/sdk-node) – a Node package for server-side use (published to NPM)
Python SDK (packages/sdk-python) – a Python library (published to PyPI) for data science and99. 
• 
scripting use 
. 

• MCP Server ( 
packages/mcp-adapter 
) – a lightweight server implementing a Model-Context 
Protocol (MCP) interface, allowing any MCP-compliant agent to call Parserator via mcp:// URIs 
.67
8
.1011
• Agent Integrations – support for frameworks like LangChain, CrewAI, Google ADK, etc., via 
specialized tools or adapters 
. (There is also an adk package stub in the repo for Google’s 
Agent Development Kit integration 
.) 
• Browser Extensions – a Chrome extension and a VS Code extension, which embed Parserator’s 
capabilities into the browser and editor, respectively 
• JetBrains IDE Plugin – (present in the repo, but not highlighted in the public launch; likely 
experimental or in-progress). 
• Web Dashboard/Hosting – a Firebase-hosted web app and API backend. The project uses Firebase 
for hosting the website and a Cloud Run or Functions API endpoint for parsing requests 
17. Domain 
and SSL are set up for parserator.com 
17, pointing to the live production service. 
This 
architecture 
is 
aimed 
at 
framework-agnostic, 
multi-environment 
support. 
For 
example, 
the 
same 
core 
engine 
powers 
a 
CLI 
tool 
( 
parserator 
parse 
CLI), 
cloud 
API, 
and 
agent 
plugins. 
All 
modules 
communicate 
via 
clear 
JSON 
schema 
definitions 
– 
a 
direct 
reflection 
of 
the 
EMA 
“open 
standards” 
doctrine 
. 
The 
design 
avoids 
any 
proprietary 
data 
formats 
or 
vendor-specific 
ML 
models, 
instead 
allowing 
the 
user 
to 
bring 
their 
own 
LLM 
API 
(Anthropic, 
OpenAI, 
etc.) 
if 
desired 
7. 
The 
Architect-Extractor 
pattern 
also 


121314
.1516
1 



.18
means 
Parserator 
itself 
remains 
LLM-provider-agnostic 
(no 
hard 
dependency 
on 
a 
single 
model 
or 
service). 
This forward-looking architecture ensures “no vendor lock-in – works with any LLM provider” 


Production-Ready Components and Stability 


As 
of 
the 
June 
13, 
2025 
v1.0.0 
public 
Parserator’s 
core 
features 
and 
several 
supporting 
components are considered production-ready 
. Key stable components include: 


release,
1920
• Core Parsing Engine – Implements the two-stage parsing workflow with robust error handling, 
validation, and high accuracy. This was a centerpiece of the 1.0.0 release 
21. The engine is designed 
for real-world inputs and has been tested on complex, “messy” data to ensure reliability. 
• Parserator SDKs – Both the Node.js and Python client libraries have been built, tested, and prepared 
for distribution. They allow developers to easily invoke the Parserator engine from their own code. 
The marketing status confirms these SDKs as “ Built and tested” for launch 
• Chrome Extension – A browser extension that lets users right-click on any text on a webpage and 
parse it via Parserator. The extension was fully built (packaged as a .zip for submission) and 
marked ready for Chrome Web Store publishing 
23. (In the public-release branch, the README even 
updates the Chrome extension section with an installation link, indicating its readiness for end-users 
15
.) 

• VS Code Extension – An editor plugin providing Parserator capabilities within Visual Studio Code. 
.22
This was also completed (packaged as parserator-1.0.0.vsix 
) and prepared for Marketplace 


publishing 
22. The extension adds developer conveniences like generating TypeScript types from 
parsed data, etc. 
24. 


• MCP Integration – The Parserator MCP server/adapter was implemented to allow universal agent 
access via the MCP protocol. At launch it was built and globally installable via NPM ( 
10parseratormcp-
server 
) 
. This opens Parserator to any agent framework that supports calling external 
tools via URI. 
• API Backend – A production API endpoint is running (Google Cloud Run/Firebase) to service parse 
requests at scale 
25. This includes secure API key management, rate limiting, and an authentication 
system for the cloud service 
20. The backend infrastructure was flagged as “production-ready” in 
the release notes and marketing checklist 
2017. 
• Documentation & Guides – All essential documentation was completed for launch: a polished 
README with usage examples and clear status notes, an extensive EMA Manifesto articulating the 
project’s guiding principles, and professional CONTRIBUTING and SECURITY guidelines 
There is also an API reference and integration guides in the docs/ folder. These materials ensure 
that developers can successfully adopt and trust the platform. 
Notably, 
the 
quality 
of 
development 
practices 
is 
high. 
The 
project 
maintainers 
established 
a 
professional 
repo 
structure 
with 
issue/PR 
templates 
and 
contribution 
standards 
. 
There 
is 
a 
Code 
of 
Conduct 
and 
Security 
policy 
in 
place, 
indicating 
a 
focus 
on 
community 
and 
safe 
handling 
of 
vulnerabilities. 
All 
code 
contributions 
(including 
those 
by 
AI 
assistants) 
follow 
a 
rule 
of 
“production-ready 
code 
only” 
– 
no 
half-
measures 
or 
throwaway 
scripts 
in 
the 
main 
branches 
. 
This 
disciplined 
approach 
(reinforced 
even 
in 
the 
CLAUDE agent instructions) has yielded a stable foundation for Parserator. 


.2627
27
28
Areas 
of 
lower 
maturity 
or 
pending 
work: 
A 
few 
components 
appear 
less 
proven 
or 
were 
omitted 
from 
the 
initial 
public 
launch. 
The 
JetBrains 
IDE 
plugin, 
for 
example, 
is 
present 
in 
the 
repository 
but 
was 
not 
mentioned 
in 
the 
release 
marketing; 
it 
may 
still 
be 
experimental 
or 
in 
need 
of 
refinement. 
Similarly, 
an 


2 



“ADK” 
(agent 
development 
kit) 
package 
is 
scaffolded 
(for 
deeper 
integration 
with 
the 
Google 
Assistant 
Developer 
Kit), 
but 
likely 
in 
early 
stages. 
These 
parts 
should 
be 
reviewed 
to 
determine 
if 
they 
are 
worth 
investing 
in 
further 
or 
if 
they 
should 
be 
set 
aside 
until 
the 
core 
sees 
more 
adoption. 
Overall, 
however, 
the 
majority 
of 
Parserator’s 
features 
that 
were 
released 
(engine 
+ 
SDKs 
+ 
extensions 
+ 
API) 
are 
robust 
and 
ready 
for 
production 
use 
9. 
The 
Changelog 
1.0.0 
affirms 
that 
all 
core 
functionality 
needed 
for 
production 
was 
included and that the project has moved from private dev to a community-ready state 
29. 


Testing Infrastructure and Analysis Implementations 


A 
major 
focus 
in 
the 
feature/june-13-testing-and-analysis 
development 
branch 
was 
to 
build 
out 
a 
comprehensive 
testing 
and 
analysis 
framework 
for 
Parserator. 
This 
branch 
(with 
commits 
on 
June 
13–14) 
introduced 
a 
suite 
of 
custom 
test 
runners 
and 
comparative 
analysis 
scripts 
aimed 
at 
validating 
Parserator’s 
performance, 
accuracy, 
and 
reliability 
in 
real-world 
scenarios. 
The 
approach 
to 
testing 
can 
be 
summarized 
as follows: 


• Real-World Scenarios: The test strategy emphasizes using “real-world messy data” (emails, 
addresses, freeform text, etc.) as test cases 
. Instead of contrived inputs, the focus is on realistic 
data that Parserator is meant to handle, ensuring the engine truly solves practical parsing problems. 
• Diverse Test Modes: Several specialized test scripts were added: 
• Basic functionality tests (e.g. simple-working-test.js 
, actual-real-test.js 
) to verify 
correct output on known inputs. 
• Edge and adversarial tests (e.g. chaos-test-runner.js 
, adversarial-test-runner.js 
) 
which feed particularly tricky or malformed inputs to the system to probe its robustness. 
30
• Comparative analysis tests – notably competitor-comparison-test.js and comparativetest-
runner.js – which likely run Parserator against alternative parsing methods or regex-based 
parsers on the same input, comparing results. This aligns with the EMA principle of transparent 
competition (openly evaluating Parserator vs. other solutions) 


• Statistical and performance tests – files like real-stats-test.js gather metrics on accuracy rates, 
token usage, and execution time across a corpus of inputs. These help quantify the “95% accuracy” 
claim and the token savings of the two-stage approach in practice. 
• Automation and Debugging Tools: Utilities such as debug-single-test.js allow step-by-step 
inspection of the Architect and Extractor outputs for a given input (useful for troubleshooting 
parsing errors on edge cases). There’s also manual-vs-parserator.js which likely was used to 
compare outputs from manual data extraction versus Parserator’s automated output, highlighting 
differences. 
• Results and Analysis: Alongside the code, the development branch introduced documentation to 
interpret test outcomes. A file TESTING_STRATEGY.md was added, presumably outlining the goals 
and methodology of the new tests (e.g. ensuring coverage of various data types and measuring 
performance over thousands of tokens). In the pre-cleanup archive, there’s also 
COMPREHENSIVE_TEST_RESULTS_AND_LAUNCH_PLAN.md which suggests a detailed report of test 
results was written, possibly summarizing that Parserator met its targets (e.g. ~95% accuracy on 
complex data, sub-3s response times) and outlining any remaining issues. This thorough testing 
effort is a strong indicator of reliability, as it provided empirically verified benchmarks and 
highlighted any gaps to address before public release. 


Validation 
of 
the 
Testing 
Approach: 
The 
testing 
infrastructure 
as 
implemented 
is 
quite 
exhaustive 
in 
scope 
– 
covering 
correctness, 
robustness 
under 
stress, 
performance, 
and 
even 
comparative 
efficacy. 
The 


.31
3 



use 
of 
actual 
data 
and 
inclusion 
of 
competitor 
comparisons 
are 
best 
practices, 
ensuring 
that 
claims 
(like 
“state-of-the-art 
accuracy” 
and 
“token 
efficiency”) 
are 
backed 
by 
evidence. 
For 
future 
maintenance, 
these 
scripts 
can 
be 
refactored 
into 
a 
more 
standard 
testing 
framework 
(e.g. 
a 
test 
suite 
that 
can 
be 
run 
via 
a 
test 
runner 
or 
CI 
pipeline). 
Currently, 
they 
exist 
as 
standalone 
Node.js 
scripts. 
Integrating 
them 
with 
a 
continuous 
integration 
setup 
(perhaps 
using 
GitHub 
Actions) 
would 
automate 
regression 
testing. 
Nonetheless, 
the 
presence 
of 
these 
analysis 
tools 
is 
extremely 
valuable: 
it 
means 
the 
team 
(including 
AI 
assistants) can continually evaluate Parserator’s performance on real data whenever changes are made. 


One 
recommendation 
is 
to 
merge 
the 
insights 
from 
these 
analysis 
scripts 
into 
formal 
unit/integration 
tests 
and 
documentation: 
-Key 
metrics 
(accuracy 
%, 
token 
savings, 
etc.) 
measured 
by 
these 
scripts 
can 
be 
added 
to 
the 
README 
or 
docs 
for 
transparency. 
-The 
TESTING_STRATEGY.md 
should 
be 
kept 
(or 
merged 
into 
a 
general 
QA 
documentation) 
to 
guide 
contributors 
on 
how 
to 
validate 
changes. 
Given 
Parserator’s 
reliance 
on 
LLM 
behavior, 
having 
clear 
testing 
protocols 
is 
important 
to 
maintain 
confidence 
in 
changes. 
-As 
Parserator 
evolves, 
the 
team 
should 
continue 
the 
practice 
of 
“accuracy 
metrics 
tracking 
with 
human 
validation” 
– 
e.g., 
manually 
reviewing 
outputs 
on 
new 
tricky 
inputs 
– 
to 
catch 
subtle 
regressions 
that 
automated tests might miss. 


Documentation Review: Goals, Architecture, Workflows, Best 
Practices 


The 
project’s 
documentation 
(across 
.md 
files 
on 
both 
main 
and 
dev 
branches) 
thoroughly 
captures 
its 
goals, 
design 
philosophy, 
and 
recommended 
workflows. 
Key 
themes 
and 
best 
practices 
from 
these 
docs 
include: 


• Project Goals & Vision: Parserator is positioned as “The Structured Data Layer for AI Agents” with a 
mission to free users from data silos. The README and EMA_MANIFESTO.md together articulate 
both the technical goal (transform unstructured text to structured JSON with high accuracy) and the 
moral goal (empower users to own and port their data) 
3233. The EMA Manifesto in particular 
reads as a passionate call to arms against vendor lock-in, establishing a principled backbone for why 
Parserator exists. All documentation consistently ties back to these core values (e.g. the extension 
descriptions even mention “Built on EMA principles – your data sovereignty matters” 
• Architectural Intentions: Several docs (README, CLAUDE.md, 
COMPLETE_SYSTEM_DOCUMENTATION.md in the archive) describe the Architect-Extractor two-
phase architecture in detail. This includes why it’s superior (token savings, higher accuracy)36and 
how it’s implemented (the prompt/response format of the SearchPlan, etc. 
3738). The CLAUDE.md 
– Parserator Project file is especially rich in architectural guidance: it lays out the entire project 
structure, interface definitions for plans, and even a phased roadmap for development 
839. This 
indicates a clear architectural blueprint was established from the start. 
• Workflows & Contribution: Parserator’s maintainers put in place strong workflow guidelines. The 
CONTRIBUTING.md (and references in CLAUDE.md) insist on high standards: thorough 
documentation, adhering to the EMA compliance checklist, writing tests for new features, etc. For 
example, the release checklist demanded an honest README update reflecting the project’s real 
status and limitations4041 – a refreshingly transparent practice. The presence of issue templates 
and PR templates (likely in the .github/ folder) and a CODE_OF_CONDUCT.md shows an 
emphasis on community and orderly collaboration 
30
).3435
4 


.27

• Agent Collaboration Best Practices: A unique aspect of this project is the explicit integration of an 
AI (Claude) in development. The CLAUDE.md files (both in this repo and a similar one in the 
vib3code project) essentially serve as “agent playbooks.” In Parserator’s CLAUDE.md, sections like 
“CRITICAL DEVELOPMENT PREFERENCES” spell out rules for the AI assistant – e.g., never produce non-
production code, always handle edge cases, ensure efficiency 
28. There’s even a section in the 
archived CLAUDE.md delineating Quality Standards (error handling, performance targets, etc.) that 
the AI should aim to meet in code contributions 
4243. This is an innovative approach to 
maintaining quality when using AI coding assistants. It effectively encodes the lead developer’s 
expectations and domain knowledge into a reference document that the assistant can consult. We 
recommend keeping an updated CLAUDE.md in the repository, serving as a living guideline for any 
AI agent (or human) contributing to the code. This file should outline the project structure, coding 
standards, testing requirements, and context on current tasks – thereby preserving context between 
sessions and ensuring continuity in agent contributions. 


• “Architect-Extractor” Pattern and EMA in Docs: The documentation frequently reinforces the 
defining features of Parserator. For instance, the README’s “How It Works” section provides a concise 
breakdown of the two stages and the results achieved 
4445. Meanwhile, the EMA Manifesto and 
EMA Compliance Checklist concretely tie those features to user benefits (e.g., “No proprietary formats – 
uses JSON, OpenAPI”, “Users can export all their data”, “Honest documentation of limitations” etc. 
46
47
). The fact that these principles appear across multiple documents (manifesto, marketing pages, 
etc.) indicates a strong alignment between the project’s technical implementation and its stated 
goals – an excellent best practice. Stakeholders and contributors can always refer back to these 
documents to understand why a certain design decision was made. 


Overall, 
the 
documentation 
paints 
a 
picture 
of 
a 
project 
that 
is 
goal-driven 
and 
methodical. 
The 
architectural 
docs 
ensure 
that 
future 
development 
stays 
coherent 
(for 
example, 
by 
following 
the 
outlined 
project 
structure 
and 
phase 
roadmap 
839). 
The 
workflow 
docs 
and 
contribution 
guidelines 
institutionalize 
best 
practices 
so 
that 
code 
quality 
and 
philosophical 
alignment 
don’t 
regress 
as 
more 
people 
get involved. 


Redundant 
Documentation: 
Given 
the 
plethora 
of 
markdown 
files 
(especially 
in 
the

Essentialvs.
backup-before-cleanupbranch),itKeep:CoredocslikeREADME.md,
CODE_OF_CONDUCT.mduser-facingguides(integrationhow-tos,APIdocs)thathelpdevelopers
’s 
prudent 
to 
streamline 
which 
docs 
remain 
actively 
maintained: 
EMA_
MANIFESTO.md 
, 
CONTRIBUTING.md 
, 
SECURITY.md 
, 
and 
are 
essential 
for 
public 
transparency 
and 
guiding 
new 
contributors. 
Also 
keep 
any 


use 
Parserator. 
-Refactor/ 
Consolidate: 
There 
are 
many 
internal 
planning 
docs 
(e.g. 
various 
LAUNCH_.md 
files, 
CHECKLISTs, 
handoff 
notes). 
These 
were 
invaluable 
during 
the 
private 
development 
and 
launch 
prep 
phase, 
but 
now 
they 
can 
be 
consolidated 
or 
archived. 
For 
example, 
a 
single 
“Project 
History 
& 
Roadmap” 
document 
could 
summarize 
the 
important 
bits 
(like 
launch 
outcomes 
and 
future 
feature 
roadmap) 
instead 
of 
dozens 
of 
individual 
files. 
Some 
content 
(like 
the 
COMPREHENSIVE_TEST_RESULTS 
or 
PERFORMANCE_BENCHMARKS) 
might 
be 
better 
summarized 
into 
a 
wiki 
page 
or 
an 
appendix 
in 
the 
repository, 
rather 
than 
kept 
as 
separate 
files 
in 
the 
main 
tree. 
-Archive: 
Documents 
that 
are 
essentially 
transaction 
logs 
of 
interactions 
with 
Claude 
(e.g., 
FINAL_HANDOFF_CLAUDE.md 
, 
or 
brainstorming 
documents), 
and 
one-off 
strategy 
papers 
(marketing 
kits, 
etc.) 
should 
be 
moved 
out 
of 
the 
active 


archive/
tagorseparatebranchlikearchive).Thekeyistodeclutterthemainbranchsothatonlyactivelyrelevantinformationispresent.Theexistenceofthebackup-before-cleanupbranchindicatesanintenttoremove
code 
workspace. 
These 
can 
live 
in 
an 
folder 
or 
simply 
be 
preserved 
in 
version 
control 
history 
(on 
a 


such 
clutter 
– 
and 
indeed 
it 
contains 
a 
trove 
of 
these 
planning 
docs 
that 
are 
no 
longer 
needed 
day-to-day. 
Maintain 
CLAUDE.md: 
As 
mentioned, 
one 
Markdown 
file 
that 
should 
remain 
and 
be 
maintained 
is 
CLAUDE.md 


5 



(or 
a 
similarly 
purposed 
Agent 
Guide). 
This 
will 
help 
any 
AI 
agent 
(like 
Claude 
or 
GitHub 
Copilot, 
etc.) 
to 
quickly 
get 
up 
to 
speed 
on 
the 
project’s 
context, 
coding 
style, 
and 
current 
objectives. 
It’s 
essentially 
an 
onboarding 
doc 
for 
an 
AI 
contributor. 
Given 
the 
success 
the 
team 
had 
co-authoring 
commits 
with 
an 
AI 
(the 
commit 
history 
shows 
Claude* 
as 
a 
co-committer 
on 
many 
key 
commits 
4849), 
continuing 
this 
practice 
could 
accelerate 
development 
– 
but 
only 
if 
the 
agent 
has 
up-to-date 
context. 
A 
well-structured 
CLAUDE.md 
(covering 
project 
overview, 
component 
descriptions, 
current 
TODOs, 
and 
pointers 
to 
tests) 
will 
preserve 
continuity 
between 
AI sessions. 


Recommendations: Restructuring for Clarity and Sustainability 


1. 
Blueprint 
of 
What 
to 
Keep, 
Refactor, 
or 
Archive 
– 
We 
recommend 
the 
following 
actions 
to 
declutter 
the 
environment and set up Parserator for its next phase: 
• Retain and Polish Core Components: All production-ready components (Core engine, Node/Python 
SDKs, API backend, Chrome & VSCode extensions, MCP adapter) should remain in the repository. 
These are the value-delivering parts of Parserator and will be the focus of maintenance and 
improvements. Ensure each of these has a clear README or documentation section. For instance, 
the packages/ could contain individual READMEs for the Node and Python SDK explaining their 
use. This makes the project more approachable to new users who may land directly in a sub-package 
via npm or PyPI. 
• Clarify Status of Peripheral Components: For components like the JetBrains plugin or the ADK 
package that were not part of the public release, decide on their fate. If they are incomplete but on 
the roadmap, mark them clearly as “Experimental” or “Work in Progress” in the README to set user 
expectations. If they are legacy experiments unlikely to be continued, consider removing them from 
the main branch (they can live in an archive branch) to reduce maintenance overhead. Every 
directory in the repo should have a purpose – pruning those that don’t avoids confusion. 
• Streamline Documentation: As discussed, consolidate the profusion of markdown docs. Focus the 
primary docs on what external users and new contributors need. Internal strategy memos and 
exhaustive launch plans can be moved out of the main tree. A single docs/ section on the main 
branch can host curated documentation: e.g., Architecture.md (architect-extractor design, schema 
format, etc.), Usage.md (quick start and integrations, perhaps expanded from README), Testing.md 
(how to run tests and interpret results), etc. Many of the existing .md files can be merged into these 
topical documents. For example, the essence of COMPREHENSIVE_LAUNCH_HANDOFF.md and 
POST_LAUNCH_ROADMAP.md could be distilled into a Roadmap section in the README or a 
ROADMAP.md. 


• Maintain the EMA Manifesto: The EMA_MANIFESTO is a differentiator and should be kept 
prominently (which it is). It might be useful to link it or summarize it in the main README for those 
who won’t immediately read the full manifesto. The principles from the manifesto should continue to 
guide development decisions (e.g., if adding a new feature, ask if it upholds EMA principles – the 
contributing guide could include this check). 
2. 
Git 
Worktree-Friendly 
Workflow 
– 
Embracing 
Git 
worktrees 
can 
significantly 
enhance 
the 
development 
workflow, 
especially 
with 
an 
AI 
agent 
in 
the 
loop. 
With 
worktrees, 
one 
can 
have 
multiple 
branches 
checked 
out in parallel in separate directories. Here’s how we suggest using them for Parserator: 
• Parallel Development & Release Branches: Maintain at least two primary long-lived branches – for 
example, main (public release) and dev (next development iteration). Using worktrees, you can 
6 



have main checked out in one folder and dev in another. This is ideal for an AI assistant: it can 
reference the stable code in main while working on new features in dev without losing context. 
The assistant could even diff between branches if needed. The question specifically references 
“public-release/main” and a feature branch – likely the user is already using this pattern. We 
encourage continuing this: treat public-release (or main 
) as the stable branch for releases, 
and do all experimental work in feature branches that can be simultaneously open. 


• Isolated Task Worktrees: If working on multiple features or fixes concurrently (especially by AI), 
create a separate worktree for each significant task branch. For example, one could have a 
feature/testing-improvements branch checked out in worktrees/testing/, and a 
feature/performance-opt in worktrees/performance/. This allows Claude (or a developer)
to switch context simply by switching directories, without juggling git stash/checkout in one working
copy. It’s a powerful way to preserve context for the AI – each worktree directory can have its own 
relevant files open and its own CLAUDE.md notes if needed. 


• Agent Coordination via CLAUDE.md: Place a CLAUDE.md (or similarly named file, perhaps 
AI_GUIDE.md 
) in the root of the repository and ensure it’s kept up-to-date. This file can be 
symlinked or copied into each worktree if variations are needed, but usually one central guide is 
enough. In it, define the project structure (as done in the archived CLAUDE.md 
8), the coding 
standards, and the current goals for the agent. For example: “We are currently working on improving 
the test framework by refactoring the scripts into a Jest suite. The relevant files are X… Ensure any new code 
passes existing tests and meets performance targets.” Also, include reminders about context (e.g. 
“Important: Do not introduce any dependency that breaks our no-lock-in principle (see EMA Manifesto).”). 
By having this in the repo, you enable any future AI agent (with access to the repo files) to quickly 
regain context even if the conversation history is lost. 


• Use Branch Protection on Main: To complement the above, protect the main (public release) 
branch so that significant changes only land there via reviewed pull requests from dev branches. This 
ensures that even if an AI is assisting, there’s a manual checkpoint before anything goes live. The dev 
branch can be a playground with the AI committing freely (the commit history shows Claude was 
directly committing in the dev/testing branch 
4950). But merges into main should be supervised. 
This workflow is worktree-friendly and keeps main stable. 
3. 
Environment 
Decluttering 
for 
Agents: 
Clean 
up 
any 
leftover 
credentials, 
config, 
or 
large 
binary 
artifacts 
that 
are 
not 
needed. 
The 
commit 
history 
shows 
some 
“CRITICAL 
SECURITY: 
remove 
exposed 
API 
keys” 
just 
before 
launch 
, 
which 
is 
good. 
Make 
sure 
the 
.gitignore 
is 
updated 
to 
prevent 
committing 
build 
artifacts 
or 
secrets. 
The 
development 
branch 
added 
certain 
build 
outputs 
(perhaps 
.vsix 
or 
.zip 
files) 
intentionally 
for 
archival, 
but 
those 
can 
be 
removed 
once 
not 
needed 
or 
moved 
to 
a 
GitHub 
Release 
instead. 
A 
tidy 
repository 
with 
only 
relevant 
files 
will 
run 
faster 
for 
the 
AI 
and 
reduce 
the 
cognitive 
load 
(for 
both 
AI 
and humans). 
4. 
Refactoring 
Opportunities: 
With 
the 
insight 
from 
testing 
and 
usage, 
identify 
parts 
of 
the 
code 
that 
could 
be 
refactored 
for 
clarity 
or 
performance. 
For 
example, 
if 
the 
two-stage 
logic 
is 
currently 
duplicated 
between 
the 
Node 
and 
Python 
implementations, 
consider 
refactoring 
into 
a 
single 
core 
(perhaps 
the 
core 
could 
be 
in 
one 
language 
and 
the 
other 
calls 
it, 
or 
abstract 
the 
plan 
format 
so 
both 
share 
the 
same 
approach). 
Ensuring 
consistency 
between 
SDKs 
is 
important. 
Another 
target 
might 
be 
the 
schema 
and 
validation 
subsystem 
– 
given 
the 
emphasis 
on 
structured 
output, 
having 
a 
strong 
schema 
definition 
(maybe 
using 
JSON 
Schema 
or 
Pydantic 
for 
Python) 
could 
be 
beneficial. 
Any 
such 
refactoring 
should 
be 
done 
in 
the 
dev branch with the new test suite providing confidence that behavior remains correct. 
51
7 



In 
summary, 
Parserator 
is 
a 
well-architected 
project 
with 
a 
clear 
mission 
and 
strong 
foundation. 
The 
most 
reliable 
components 
– 
the 
parsing 
engine 
and 
its 
client 
interfaces 
– 
are 
production-ready 
and 
should 
remain 
the 
focus. 
Testing 
and 
analysis 
improvements 
from 
the 
June 
13 
feature 
branch 
greatly 
enhance 
confidence 
in 
the 
system; 
integrating 
those 
into 
regular 
development 
workflow 
(and 
CI) 
will 
ensure 
Parserator 
maintains 
its 
95% 
accuracy 
promise 
in 
future 
updates. 
By 
trimming 
away 
redundant 
files 
and 
organizing 
around 
a 
CLAUDE.md-guided, 
worktree-enabled 
workflow, 
the 
project 
can 
make 
it 
easier 
for 
both 
human 
and 
AI 
contributors 
to 
coordinate. 
This 
will 
support 
rapid, 
safe 
iteration 
on 
new 
features 
(perhaps 
multi-modal 
parsing, 
deeper 
agent 
integrations, 
etc., 
per 
the 
roadmap) 
without 
losing 
sight 
of 
the 
core principles that define Parserator. 

Going 
forward, 
every 
change 
should 
continue 
to 
be 
measured 
against 
the 
EMA 
principles 
checklist 
(perhaps 
even 
as 
an 
item 
in 
PR 
templates) 
to 
preserve 
the 
project’s 
unique 
value 
proposition. 
With 
these 
structural 
and 
process 
adjustments, 
Parserator 
will 
be 
well-positioned 
to 
grow 
its 
community 
and 
functionality 
while 
remaining stable, transparent, and user-first – exactly as intended. 


CLAUDE.md128142830363738394243
https://github.com/Domusgpt/parserator-archive/blob/a9b6f0d882e38df3be43459050fadfb2194d0759/CLAUDE.md 


315161824
GitHub - Domusgpt/parserator at public-release 


https://github.com/Domusgpt/parserator/tree/public-release 


456731334647
EMA_MANIFESTO.md 


https://github.com/Domusgpt/parserator/blob/c60f6237ef602349bedccc952ac74e3b742df497/EMA_MANIFESTO.md 


91722233435
MARKETING_STATUS_UPDATE.md 


https://github.com/Domusgpt/parserator/blob/c60f6237ef602349bedccc952ac74e3b742df497/MARKETING_STATUS_UPDATE.md 


10111213324445
GitHub - Domusgpt/parserator: Parserator - Private Development Repository 


https://github.com/Domusgpt/parserator 


192021252627294041
CHANGELOG.md 


https://github.com/Domusgpt/parserator/blob/c60f6237ef602349bedccc952ac74e3b742df497/CHANGELOG.md 


48495051
Commits · Domusgpt/parserator · GitHub 


https://github.com/Domusgpt/parserator/commits/june-13-testing-and-analysis/ 


8 



